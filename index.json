[{"authors":["admin"],"categories":null,"content":"I am a postdoctoral scholar at Caltech working with Antonio Rangel. I received my PhD from Stanford where I worked with Russell Poldrack. I study neural correlates of decision-making, particularly decisions that involve self-control. My work has a strong emphasis on methodological rigor and reproducibility. To this end I investigate the psychometric properties of measures in the field and build statistical models for neuroimaging data that account for the brain’s network structure.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://zenkavi.github.io/author/a.-zeynep-enkavi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a.-zeynep-enkavi/","section":"authors","summary":"I am a postdoctoral scholar at Caltech working with Antonio Rangel. I received my PhD from Stanford where I worked with Russell Poldrack. I study neural correlates of decision-making, particularly decisions that involve self-control.","tags":null,"title":"A. Zeynep Enkavi","type":"authors"},{"authors":["A. Zeynep Enkavi","Russell A. Poldrack"],"categories":null,"content":"","date":1593129600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593129600,"objectID":"f3ae93355a6a6c1f31f5b265fb8140ce","permalink":"https://zenkavi.github.io/publication/jpa2020/","publishdate":"2020-06-26T00:00:00Z","relpermalink":"/publication/jpa2020/","section":"publication","summary":"At the heart of science is measurement, and the quality of measurements limits the quality of the resulting conclusions.  In psychiatric research, the most common measurement has traditionally been through self-report, using scales that assess the degree and frequency of psychiatric symptoms.  However, self-report has largely been eschewed within biological and computational psychiatry for lacking the ability to provide mechanistic insights into the disorders in question. Instead, researchers now focus primarily on task-based measures of behavior combined with model-based analyses. This approach is thought to allow a deeper insight into the underlying neural and computational mechanisms whose dysfunction ultimately gives rise to psychiatric symptoms and illness. Indeed, the RDoC framework is explicitly built around these underlying neurocognitive dimensions. The measures proposed in the framework are meant to assess the function of mechanisms instead of or in addition to the frequency and severity of symptoms.  The subjective nature of self-report, compared to the seemingly objective nature of cognitive tasks in combination with sophisticated computational models, has led many researchers to move away from the former. Measurement quality issues have long been of concern within clinical psychology and psychiatry. Yet the focus has been on challenges posed by clinical conditions due to e.g. medication effects, comorbid cognitive deficits or differences in motor abilities. A growing body of work in cognitive psychology points towards more fundamental challenges to measurement quality from self-report and behavioral tasks regardless of clinical conditions. One often neglected consideration in the context of behavioral measures and model-based assessments is their psychometric characteristics.  Whereas the publication of a novel self-report scale is necessarily accompanied by a demonstration of both its reliability (e.g. through test-retest and/or internal consistency measures) and its validity (e.g. convergent, divergent, and predictive validity), it is exceedingly rare to see psychometric assessments of cognitive measures and computational models. Here we discuss recent work that has raised pointed questions regarding the psychometric features of many task-based measures and model-based analyses that are the basis for biological and computational psychiatry.  We hope that renewed interest in psychometrics will help improve the potential utility of these measures for understanding the foundations of mental illness.  Although neuroimaging measures are central to these fields as well, we do not address those in the present paper; however, recent work has raised important questions about the reliability of fMRI results as well. Large-scale, computerized behavioral studies and consortia for task-based biological measures have enabled researchers to test psychometric properties of broad sets of both self-report and task measures. These efforts reveal concerns about the construct validity of various cognitive processes thought to underlie numerous clinical conditions, the poor psychometric features of cognitive task measures and limitations on the generalizability of model-based analyses. In this commentary we provide a summary of some of these findings, discuss their implications for psychiatry research and present suggestions to improve the field.","tags":null,"title":"Implications of the lacking relationship between cognitive task and self report measures for psychiatry","type":"publication"},{"authors":["Gina L. Mazza","Heather L. Smyth","Patrick G. Bissett","Jessica R. Canning","Ian W. Eisenberg","A. Zeynep Enkavi","Oscar Gonzalez","Sunny Jung Kim","Stephen A. Metcalf","Felix Muniz","William E. Pelham III","Emily A. Scherer","Matthew J. Valente","Haiyi Xie","Russell A. Poldrack","Lisa A. Marsch","David P. MacKinnon"],"categories":null,"content":"","date":1585612800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585612800,"objectID":"96ddca847e36a56f197b88db944aeb38","permalink":"https://zenkavi.github.io/publication/biopsyc2020/","publishdate":"2020-03-31T00:00:00Z","relpermalink":"/publication/biopsyc2020/","section":"publication","summary":"Self-regulation is studied across various disciplines, including personality, social, cognitive, health, developmental, and clinical psychology; psychiatry; neuroscience; medicine; pharmacology; and economics. Widespread interest in self-regulation has led to confusion regarding both the constructs within the nomological network of self-regulation and the measures used to assess these constructs. To facilitate the integration of cross-disciplinary measures of self-regulation, we estimated product-moment and distance correlations among 60 cross-disciplinary measures of self-regulation (23 self-report surveys, 37 cognitive tasks) and measures of health and substance use based on 522 participants. The correlations showed substantial variability, though the surveys demonstrated greater convergent validity than did the cognitive tasks. Variables derived from the surveys only weakly correlated with variables derived from the cognitive tasks (M = .049, range = .000 to .271 for the absolute value of the product-moment correlation; M = .085, range = .028 to .241 for the distance correlation), thus challenging the notion that these surveys and cognitive tasks measure the same construct. We conclude by outlining several potential uses for this publicly available database of correlations.","tags":null,"title":"Correlation Database of 60 Cross-Disciplinary Surveys and Cognitive Tasks Assessing Self-Regulation","type":"publication"},{"authors":["Ian W. Eisenberg","Patrick G. Bissett","A. Zeynep Enkavi","Jamie Li","David P. MacKinnon","Lisa A. Marsch","Russell A. Poldrack"],"categories":null,"content":"","date":1558656e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558656e3,"objectID":"89273422c7e432226f8b5e0b0ddd62bb","permalink":"https://zenkavi.github.io/publication/natcom2019/","publishdate":"2019-05-24T00:00:00Z","relpermalink":"/publication/natcom2019/","section":"publication","summary":"Psychological sciences have identified a wealth of cognitive processes and behavioral phenomena, yet struggle to produce cumulative knowledge. Progress is hamstrung by siloed scientific traditions and a focus on explanation over prediction, two issues that are particularly damaging for the study of multifaceted constructs like self-regulation. Here, we derive a psychological ontology from a study of individual differences across a broad range of behavioral tasks, self-report surveys, and self-reported real-world outcomes associated with self-regulation. Though both tasks and surveys putatively measure self-regulation, they show little empirical relationship. Within tasks and surveys, however, the ontology identifies reliable individual traits and reveals opportunities for theoretic synthesis. We then evaluate predictive power of the psychological measurements and find that while surveys modestly and heterogeneously predict real-world outcomes, tasks largely do not. We conclude that self-regulation lacks coherence as a construct, and that data-driven ontologies lay the groundwork for a cumulative psychological science.","tags":null,"title":"Uncovering the structure of self-regulation through data-driven ontology discovery","type":"publication"},{"authors":["A. Zeynep Enkavi","Ian W. Eisenberg","Patrick G. Bissett","Gina L. Mazza","David P. MacKinnon","Lisa A. Marsch","Russell A. Poldrack"],"categories":null,"content":"","date":1552953600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552953600,"objectID":"71fb0492c081c8e0a124545b4e8c0226","permalink":"https://zenkavi.github.io/publication/pnas2019/","publishdate":"2019-03-19T00:00:00Z","relpermalink":"/publication/pnas2019/","section":"publication","summary":"The ability to regulate behavior in service of long-term goals is a widely studied psychological construct known as self-regulation. This wide interest is in part due to the putative relations between self-regulation and a range of real-world behaviors. Self-regulation is generally viewed as a trait, and individual differences are quantified using a diverse set of measures, including self-report surveys and behavioral tasks. Accurate characterization of individual differences requires measurement reliability, a property frequently characterized in self-report surveys, but rarely assessed in behavioral tasks. We remedy this gap by (i) providing a comprehensive literature review on an extensive set of self-regulation measures and (ii) empirically evaluating test–retest reliability of this battery in a new sample. We find that dependent variables (DVs) from self-report surveys of self-regulation have high test–retest reliability, while DVs derived from behavioral tasks do not. This holds both in the literature and in our sample, although the test–retest reliability estimates in the literature are highly variable. We confirm that this is due to differences in between-subject variability. We also compare different types of task DVs (e.g., model parameters vs. raw response times) in their suitability as individual difference DVs, finding that certain model parameters are as stable as raw DVs. Our results provide greater psychometric footing for the study of self-regulation and provide guidance for future studies of individual differences in this domain.","tags":null,"title":"Large-scale analysis of test–retest reliabilities of self-regulation measures","type":"publication"},{"authors":["A. Zeynep Enkavi","Ian W. Eisenberg","Patrick G. Bissett","Gina L. Mazza","David P. MacKinnon","Lisa A. Marsch","Russell A. Poldrack"],"categories":null,"content":"","date":1552953600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552953600,"objectID":"7df4657589a37bbe62d7c40305719268","permalink":"https://zenkavi.github.io/publication/pnas2019_2/","publishdate":"2019-03-19T00:00:00Z","relpermalink":"/publication/pnas2019_2/","section":"publication","summary":"In their Letter to the Editor, Friedman and Banich (1) suggest we (2) “overstate” the higher suitability of dependent variables (DVs) derived from surveys for individual difference analyses. We appreciate this opportunity for a continued discussion regarding the measurement of self-regulation. However, their critiques (1) do not provide evidence against the higher reliability of survey DVs, which is central to our conclusions. Instead, they levy criticisms addressed in another of our papers (3).","tags":null,"title":"Reply to Friedman and Banich: Right measures for the research question","type":"publication"},{"authors":["A. Zeynep Enkavi","Bernd Weber","Iris Zweyer","Jan Wagner","Christian E. Elger","Elke U. Weber","Eric J. Johnson"],"categories":null,"content":"","date":1513555200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1513555200,"objectID":"aca694db7cfae808254e8c08aaae21b3","permalink":"https://zenkavi.github.io/publication/scirep2017/","publishdate":"2017-12-18T00:00:00Z","relpermalink":"/publication/scirep2017/","section":"publication","summary":"Consistent decisions are intuitively desirable and theoretically important for utility maximization. Neuroeconomics has established the neurobiological substrate of value representation, but brain regions that provide input to this network is less explored. The constructed-preference tradition within behavioral decision research gives a critical role to associative cognitive processes, suggesting a hippocampal role in making consistent decisions. We compared the performance of 31 patients with mediotemporal lobe (MTL) epilepsy and hippocampal lesions, 30 patients with extratemporal lobe epilepsy, and 30 healthy controls on two tasks: binary choices between candy bars based on their preferences and a number-comparison control task where the larger number is chosen. MTL patients made more inconsistent choices than the other two groups for the value-based choice but not the number-comparison task. These inconsistencies correlated with the volume of compromised hippocampal tissue. These results add to increasing evidence on a critical involvement of the MTL in preference construction and value-based choices.","tags":null,"title":"Evidence for hippocampal dependence of value-based decisions","type":"publication"},{"authors":["Ian W.Eisenberg","Patrick G.Bissett","Jessica R.Canning","Jesse Dallery","A. Zeynep Enkavi","Susan Whitfield-Gabrieli","Oscar Gonzalez","Alan I. Green","Mary Ann Greene","Michaela Kiernan","Sunny Jung Kim","Jamie Li","Michael R.Lowe","Gina L. Mazza","Stephen A. Metcalf","Lisa Onken","Sadev S. Parikh","EllenPeters","Judith J. Prochaska","Emily A. Scherer","Luke E. Stoeckel","Matthew J. Valente","Jialing Wu","Haiyi Xie","David P. MacKinnon","Lisa A. Marsch","Russell A. Poldrack"],"categories":null,"content":"","date":1507161600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1507161600,"objectID":"3d80a8b33f0ce7e11ebe3e28fc956931","permalink":"https://zenkavi.github.io/publication/brt2017/","publishdate":"2017-10-05T00:00:00Z","relpermalink":"/publication/brt2017/","section":"publication","summary":"Self-regulation is a broad construct representing the general ability to recruit cognitive, motivational and emotional resources to achieve long-term goals. This construct has been implicated in a host of health-risk behaviors, and is a promising target for fostering beneficial behavior change. Despite its clear importance, the behavioral, psychological and neural components of self-regulation remain poorly understood, which contributes to theoretical inconsistencies and hinders maximally effective intervention development. We outline a research program that seeks to define a neuropsychological ontology of self-regulation, articulating the cognitive components that compose self-regulation, their relationships, and their associated measurements. The ontology will be informed by two large-scale approaches to assessing individual differences: first purely behaviorally using data collected via Amazon's Mechanical Turk, then coupled with neuroimaging data collected from a separate population. To validate the ontology and demonstrate its utility, we will then use it to contextualize health risk behaviors in two exemplar behavioral groups: overweight/obese adults who binge eat and smokers. After identifying ontological targets that precipitate maladaptive behavior, we will craft interventions that engage these targets. If successful, this work will provide a structured, holistic account of self-regulation in the form of an explicit ontology, which will better clarify the pattern of deficits related to maladaptive health behavior, and provide direction for more effective behavior change interventions.","tags":null,"title":"Applying novel technologies and methods to inform the ontology of self-regulation","type":"publication"},{"authors":["Vanessa V. Sochat","Ian W. Eisenberg","A. Zeynep Enkavi","Jamie Li","Patrick G. Bissett","Russell A. Poldrack"],"categories":null,"content":"","date":1461628800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461628800,"objectID":"f8f855a501bdadb506343e713bac32cb","permalink":"https://zenkavi.github.io/publication/fip2016/","publishdate":"2016-04-26T00:00:00Z","relpermalink":"/publication/fip2016/","section":"publication","summary":"The administration of behavioral and experimental paradigms for psychology research is hindered by lack of a coordinated effort to develop and deploy standardized paradigms. While several frameworks (Mason and Suri, 2011; McDonnell et al., 2012; de Leeuw, 2015; Lange et al., 2015) have provided infrastructure and methods for individual research groups to develop paradigms, missing is a coordinated effort to develop paradigms linked with a system to easily deploy them. This disorganization leads to redundancy in development, divergent implementations of conceptually identical tasks, disorganized and error-prone code lacking documentation, and difficulty in replication. The ongoing reproducibility crisis in psychology and neuroscience research (Baker, 2015; Open Science Collaboration, 2015) highlights the urgency of this challenge: reproducible research in behavioral psychology is conditional on deployment of equivalent experiments. A large, accessible repository of experiments for researchers to develop collaboratively is most efficiently accomplished through an open source framework. Here we present the Experiment Factory, an open source framework for the development and deployment of web-based experiments. The modular infrastructure includes experiments, virtual machines for local or cloud deployment, and an application to drive these components and provide developers with functions and tools for further extension. We release this infrastructure with a deployment (http://www.expfactory.org) that researchers are currently using to run a set of over 80 standardized web-based experiments on Amazon Mechanical Turk. By providing open source tools for both deployment and development, this novel infrastructure holds promise to bring reproducibility to the administration of experiments, and accelerate scientific progress by providing a shared community resource of psychological paradigms.","tags":null,"title":"The experiment factory: standardizing behavioral experiments","type":"publication"},{"authors":["Ye Li","Jie Gao","A. Zeynep Enkavi","Lisa Zaval","Elke U. Weber","Eric J. Johnson"],"categories":null,"content":"","date":1420502400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420502400,"objectID":"016f5d34b23608e6894e2ebf1c5443f0","permalink":"https://zenkavi.github.io/publication/pnas2015/","publishdate":"2015-01-06T00:00:00Z","relpermalink":"/publication/pnas2015/","section":"publication","summary":"Age-related deterioration in cognitive ability may compromise the ability of older adults to make major financial decisions. We explore whether knowledge and expertise accumulated from past decisions can offset cognitive decline to maintain decision quality over the life span. Using a unique dataset that combines measures of cognitive ability (fluid intelligence) and of general and domain-specific knowledge (crystallized intelligence), credit report data, and other measures of decision quality, we show that domain-specific knowledge and expertise provide an alternative route for sound financial decisions. That is, cognitive aging does not spell doom for financial decision-making in domains where the decision maker has developed expertise. These results have important implications for public policy and for the design of effective interventions and decision aids.","tags":null,"title":"Sound credit scores and financial decisions despite cognitive aging","type":"publication"},{"authors":null,"categories":null,"content":"Experiment Factory\nExperiment Factory is an “an open source framework for the development and deployment of web-based experiments.” We developed this platform to run our large scale multiwave online experiments. It consists of a Python application and a repository of behavioral experiments and surveys coded in javascript using jsPsych (de Leeuw, 2015). As we hoped for it has been used by other researchers for their experiments as well.\nReproducible meta-analysis\nThis is a prospective project in a very early stage that I want to continue working on in the near future. The broad aim is to build tools that would enable researchers to conduct fast and reproducible meta-analyses. Some components I am currently considering include:\nStandardized data format for meta-analysis search results Browser widget that saves dated search queries and results Online platform to upload published/unpublished datasets Result and keyword extractor from published papers Crowd-sourced tagging system of extracted results Search engine to filter extracted results References:\nKliemann, D., Adolphs, R., Armstrong, T., Galdi, P., Kahn, D.A., Rusch, T., Enkavi, A.Z., Liang, D., Lograsso, S., Zhu, W. and Yu, R., (2022). Caltech Conte Center, a multimodal data resource for exploring social cognition and decision-making. Scientific Data, 9(1), 138.\nEnkavi, A. Z., Poldrack, R. A. (2020). Implications of the lacking relationship between cognitive task and self report measures for psychiatry. Biological Psychiatry: Cognitive Neuroscience and Neuroimaging, 6 (7), 670-672.\nMazza, G. L., Smyth, H. L., Bissett, P. G, Canning, J. R., Eisenberg, I. W., Enkavi, A. Z., Gonzalez, O., Kim, S. J., Metcalf, S. A., Muniz, F., Pelham III, W. E., Scherer E. A., Valente, M. J., Xie, H., Poldrack, R. A., Marsch, L. A., MacKinnon, D. P. (2020). Correlation Database of 60 Cross-Disciplinary Surveys and Cognitive Tasks Assessing Self-Regulation. Journal of Personality Assessment, 103 (2), 238-245.\nSochat, V. V., Eisenberg, I. W., Enkavi, A. Z., Li, J., Bissett, P. G., \u0026amp; Poldrack, R. A. (2016). The experiment factory standardizing behavioral experiments. Frontiers in psychology, 7, 610.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"1b142862a606e9159f9fc72711a2c97f","permalink":"https://zenkavi.github.io/project/reproducible-science/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/reproducible-science/","section":"project","summary":"How can we make cognitive neuroscience more reproducible?","tags":["Reproducible science"],"title":"Reproducible science","type":"project"},{"authors":null,"categories":null,"content":"Ontological structure of the space\nPsychology is rich with related constructs on self-regulation (e.g. impulsivity, self-control, inhibition etc.) and even richer in behavioral tasks and surveys to measure these putative constructs. It does lack, however, a clear understanding of how these constructs and the measures relate to each other. As part of the Science of Behavior Change initiative we addressed this question by running a large battery (35 tasks, 23 surveys) on a large sample (n=552).\nTrait measures of self-regulation\nTasked with finding the best ‘behavioral assays’ of self-regulation we examined the stability of all the measures from our large battery of tasks and surveys. We followed up with 150 of our participants who completed our battery and asked them to complete it a second time 2-4 months after their initial completion. Additionally we completed a detailed analysis of the literature on the retest reliability of all our measures for comparability with our dataset. We found that the literature was noisy to begin with and that the survey measures were more reliable than task measures.\nReferences:\nMazza, G. L., Smyth, H. L., Bissett, P. G, Canning, J. R., Eisenberg, I. W., Enkavi, A. Z., Gonzalez, O., Kim, S. J., Metcalf, S. A., Muniz, F., Pelham III, W. E., Scherer E. A., Valente, M. J., Xie, H., Poldrack, R. A., Marsch, L. A., MacKinnon, D. P. (2020). Correlation Database of 60 Cross-Disciplinary Surveys and Cognitive Tasks Assessing Self-Regulation. Journal of Personality Assessment, 103 (2), 238-245.\nEnkavi, A. Z., Eisenberg, I. W., Bissett, P. G., Mazza, G. L., MacKinnon, D. P., Marsch, L. A., \u0026amp; Poldrack, R. A. (2019). Reply to Friedman and Banich: Right measures for the research question. Proceedings of the National Academy of Sciences, 116(49), 24398-24399.\nEisenberg, I. W., Bissett, P. G., Enkavi, A. Z., Li, J., MacKinnon, D. P., Marsch, L. A., \u0026amp; Poldrack, R. A. (2019). “Uncovering the structure of self-regulation through data-driven ontology discovery.” Nature communications, 10(1), 2319.\nEnkavi, A. Z., Eisenberg, I. W., Bissett, P. G., Mazza, G. L., MacKinnon, D. P., Marsch, L. A., \u0026amp; Poldrack, R. A. (2019). Large-scale analysis of test–retest reliabilities of self-regulation measures. Proceedings of the National Academy of Sciences, 116(12), 5472-5477.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"924c9a90f211cb1a2b3650b1ca96635b","permalink":"https://zenkavi.github.io/project/self-control/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/self-control/","section":"project","summary":"How should we measure self-control?","tags":"","title":"Self-control","type":"project"}]